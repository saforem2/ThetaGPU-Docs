{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data Science Software Availability On Theta GPU, currently we support the major deep learning frameworks through two paths: singularity containers, based off of Nvidia's docker containers, and through bare-metal source builds. The bare-metal builds are so far only for tensorflow 2.X, with plans to support pytorch soon. Tensorflow 1.X is supported only via Nvidia's containers at this time. Containers As of now, the nvidia containers with tensorflow 1, 2 and pytorch built against cuda11, cudnn8 are available in singularity format here: $ ls /lus/theta-fs0/projects/datascience/thetaGPU/containers/ pytorch_20.08-py3.sif tf1_20.08-py3.sif tf2_20.08-py3.sif Execute a container interactively like this: $ singularity exec --nv -B /lus:/lus /lus/theta-fs0/projects/datascience/thetaGPU/containers/tf1_20.08-py3.sif bash","title":"Home"},{"location":"#data-science-software-availability","text":"On Theta GPU, currently we support the major deep learning frameworks through two paths: singularity containers, based off of Nvidia's docker containers, and through bare-metal source builds. The bare-metal builds are so far only for tensorflow 2.X, with plans to support pytorch soon. Tensorflow 1.X is supported only via Nvidia's containers at this time.","title":"Data Science Software Availability"},{"location":"#containers","text":"As of now, the nvidia containers with tensorflow 1, 2 and pytorch built against cuda11, cudnn8 are available in singularity format here: $ ls /lus/theta-fs0/projects/datascience/thetaGPU/containers/ pytorch_20.08-py3.sif tf1_20.08-py3.sif tf2_20.08-py3.sif Execute a container interactively like this: $ singularity exec --nv -B /lus:/lus /lus/theta-fs0/projects/datascience/thetaGPU/containers/tf1_20.08-py3.sif bash","title":"Containers"},{"location":"GPU%20Monitoring/","text":"GPU Monitoring Each GPU on ThetaGPU hosts 8 A100 GPUs. You can see information about these GPUs via the command nvidia-smi . Each GPU has 40Gb of on-GPU memory. When you run applications, you will know the GPU is in use when you see the memory increase and the GPU Utilization will be non-zero. You can target a specific GPU with nvidia-smi -i 0 for the first GPU, for example. GPU Selection In many application codes, you may want to specifiy which GPU is used. This is particular important in node-sharing applications where each GPU is running it's own code, which can be either in data-parallel model training, workflow based throughput jobs, etc. You can control individual process launches with: # Specify to run only on GPU 4: export CUDA_VISIBLE_DEVICES = 4 # Let your application see GPUS 0, 1, and 7: export CUDA_VISIBLE_DEVICES = \"0,1,7\" In these cases, the GPU orderings will appear as a consecutive list starting with 0. From inside an application, many software frameworks have ability to let you target specific GPUs, including tensorflow and pytorch: Tensorflow Pytorch","title":"GPU Monitoring"},{"location":"GPU%20Monitoring/#gpu-monitoring","text":"Each GPU on ThetaGPU hosts 8 A100 GPUs. You can see information about these GPUs via the command nvidia-smi . Each GPU has 40Gb of on-GPU memory. When you run applications, you will know the GPU is in use when you see the memory increase and the GPU Utilization will be non-zero. You can target a specific GPU with nvidia-smi -i 0 for the first GPU, for example.","title":"GPU Monitoring"},{"location":"GPU%20Monitoring/#gpu-selection","text":"In many application codes, you may want to specifiy which GPU is used. This is particular important in node-sharing applications where each GPU is running it's own code, which can be either in data-parallel model training, workflow based throughput jobs, etc. You can control individual process launches with: # Specify to run only on GPU 4: export CUDA_VISIBLE_DEVICES = 4 # Let your application see GPUS 0, 1, and 7: export CUDA_VISIBLE_DEVICES = \"0,1,7\" In these cases, the GPU orderings will appear as a consecutive list starting with 0. From inside an application, many software frameworks have ability to let you target specific GPUs, including tensorflow and pytorch: Tensorflow Pytorch","title":"GPU Selection"},{"location":"Singularity%20Containers/","text":"Nvidia Containers Nvidia delivers docker containers that contain their latest release of CUDA, tensorflow, pytorch, etc. You can see the full support matrix for all of their containers here: https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html Docker is not runnable on ALCF's ThetaGPU system for most users, but singularity is. To convert one of these images to signularity you can use the following command: singularity build $OUTPUT_NAME $NVIDIA_CONTAINER_LOCATION where $OUTPUT_NAME is typically of the form tf2_20.09-py3.simg and $NVIDIA_CONTAINER_LOCATION can be a docker url such as docker://nvcr.io/nvidia/tensorflow:20.09-tf2-py3 You can find the latest containers from nvidia here: - Tensorflow 1 and 2: https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow - Pytorch: https://ngc.nvidia.com/catalog/containers/nvidia:pytorch For your convienience, we've converted these containers to singularity and they are available for August, 2020 and September 2020 here: /lus/theta-fs0/software/thetagpu/nvidia-containers/ To extend the python libraries in these containers, please see https://github.com/argonne-lcf/ThetaGPU-Docs/blob/master/building_python_packages.md For running with these containers, please see [NEEDS LINK] For issues with these containers, please email support@alcf.anl.gov .","title":"Singularity Containers"},{"location":"Singularity%20Containers/#nvidia-containers","text":"Nvidia delivers docker containers that contain their latest release of CUDA, tensorflow, pytorch, etc. You can see the full support matrix for all of their containers here: https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html Docker is not runnable on ALCF's ThetaGPU system for most users, but singularity is. To convert one of these images to signularity you can use the following command: singularity build $OUTPUT_NAME $NVIDIA_CONTAINER_LOCATION where $OUTPUT_NAME is typically of the form tf2_20.09-py3.simg and $NVIDIA_CONTAINER_LOCATION can be a docker url such as docker://nvcr.io/nvidia/tensorflow:20.09-tf2-py3 You can find the latest containers from nvidia here: - Tensorflow 1 and 2: https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow - Pytorch: https://ngc.nvidia.com/catalog/containers/nvidia:pytorch For your convienience, we've converted these containers to singularity and they are available for August, 2020 and September 2020 here: /lus/theta-fs0/software/thetagpu/nvidia-containers/ To extend the python libraries in these containers, please see https://github.com/argonne-lcf/ThetaGPU-Docs/blob/master/building_python_packages.md For running with these containers, please see [NEEDS LINK] For issues with these containers, please email support@alcf.anl.gov .","title":"Nvidia Containers"},{"location":"building_python_packages/","text":"To build python packages for Theta GPU, there are two options: build on top of a bare-metal build (currently not available, but coming soon) or build on top of (and within) a singularity container. Additionally, you can build a new container from nvidia's docker images. Building on top of a container At the moment, you will need two shells to do this: have one open on a login node (for example, thetaloginN , and one open on a compute node ( thetagpuN ). First, start the container in interactive mode: singularity exec -B /lus:/lus --nv /lus/theta-fs0/projects/datascience/thetaGPU/containers/pytorch_20.08-py3.sif bash From here, you can create a virtual env for installation: export VENV_LOCATION = /path/to/virtualenv # replace this with your path! python -m venv --system-site-packages $VENV_LOCATION Note: sometimes, the venv package is available and if not, you can try python -m virtualenv . If neither are available, you can install it in your user directory: pip install --user virtualenv and it should work. Next time you log in, you'll have to start the container, and then run source $VENV_LOCATION/bin/activate to re-enable your installed packages. Reaching the outside world for pip packages You'll notice right away when you try to pip install you can not, because the connection fails. You can, however, go through a proxy server for pip by enabling these variables: export HTTP_PROXY = http://theta-proxy.tmi.alcf.anl.gov:3128 export HTTPS_PROXY = https://theta-proxy.tmi.alcf.anl.gov:3128 Now, you can pip install your favorite packages: pip install mpi4py Building custom packages Most packages (hdf5, for example, or python packages) can be built and installed into your virtual env. Here are two common examples that aren't currently part of the pytorch container that may be useful. HDF5 You can find the source code for hdf5 on their website https://www.hdfgroup.org/downloads/hdf5/source-code/. When downloaded and un-tarred, cd to the directory and run ./configure --prefix = $VENV_LOCATION # Add any other configuration arguments make -j 64 make install This should get you hdf5! For example, after this: (pytorch_20.08) Singularity> which h5cc /home/cadams/ThetaGPU/venvs/pytorch_20.08/bin/h5cc # This is my virtualenv, success! Horovod Horovod is useful for distributed training. To use it, you need it enabled within the container. git clone https://github.com/horovod/horovod.git cd horovod git submodule update --init python setup.py build python setup.py install This should install horovod within your container.","title":"Building Python Packages"},{"location":"building_python_packages/#building-on-top-of-a-container","text":"At the moment, you will need two shells to do this: have one open on a login node (for example, thetaloginN , and one open on a compute node ( thetagpuN ). First, start the container in interactive mode: singularity exec -B /lus:/lus --nv /lus/theta-fs0/projects/datascience/thetaGPU/containers/pytorch_20.08-py3.sif bash From here, you can create a virtual env for installation: export VENV_LOCATION = /path/to/virtualenv # replace this with your path! python -m venv --system-site-packages $VENV_LOCATION Note: sometimes, the venv package is available and if not, you can try python -m virtualenv . If neither are available, you can install it in your user directory: pip install --user virtualenv and it should work. Next time you log in, you'll have to start the container, and then run source $VENV_LOCATION/bin/activate to re-enable your installed packages.","title":"Building on top of a container"},{"location":"building_python_packages/#reaching-the-outside-world-for-pip-packages","text":"You'll notice right away when you try to pip install you can not, because the connection fails. You can, however, go through a proxy server for pip by enabling these variables: export HTTP_PROXY = http://theta-proxy.tmi.alcf.anl.gov:3128 export HTTPS_PROXY = https://theta-proxy.tmi.alcf.anl.gov:3128 Now, you can pip install your favorite packages: pip install mpi4py","title":"Reaching the outside world for pip packages"},{"location":"building_python_packages/#building-custom-packages","text":"Most packages (hdf5, for example, or python packages) can be built and installed into your virtual env. Here are two common examples that aren't currently part of the pytorch container that may be useful.","title":"Building custom packages"},{"location":"building_python_packages/#hdf5","text":"You can find the source code for hdf5 on their website https://www.hdfgroup.org/downloads/hdf5/source-code/. When downloaded and un-tarred, cd to the directory and run ./configure --prefix = $VENV_LOCATION # Add any other configuration arguments make -j 64 make install This should get you hdf5! For example, after this: (pytorch_20.08) Singularity> which h5cc /home/cadams/ThetaGPU/venvs/pytorch_20.08/bin/h5cc # This is my virtualenv, success!","title":"HDF5"},{"location":"building_python_packages/#horovod","text":"Horovod is useful for distributed training. To use it, you need it enabled within the container. git clone https://github.com/horovod/horovod.git cd horovod git submodule update --init python setup.py build python setup.py install This should install horovod within your container.","title":"Horovod"},{"location":"mpi/","text":"Launching a Singularity container with MPI 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #!/bin/bash SINGULARITYBIN = $( which singularity ) CONTAINER = ${ HOME } /singularity_images/hpl-mofed5-cuda11runtime_verbs.sif OMPI_MCA_orte_launch_agent = $( cat <<EOF $SINGULARITYBIN run --nv $CONTAINER orted EOF ) export SINGULARITYENV_OMPI_MCA_orte_launch_agent = ${ OMPI_MCA_orte_launch_agent } $SINGULARITYBIN run --nv --cleanenv \\ $CONTAINER \\ mpirun \\ -H thetagpu01:1,thetagpu02:1 \\ -n 2 \\ --mca plm_rsh_args \"-p22\" \\ hostname","title":"MPI"},{"location":"mpi/#launching-a-singularity-container-with-mpi","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #!/bin/bash SINGULARITYBIN = $( which singularity ) CONTAINER = ${ HOME } /singularity_images/hpl-mofed5-cuda11runtime_verbs.sif OMPI_MCA_orte_launch_agent = $( cat <<EOF $SINGULARITYBIN run --nv $CONTAINER orted EOF ) export SINGULARITYENV_OMPI_MCA_orte_launch_agent = ${ OMPI_MCA_orte_launch_agent } $SINGULARITYBIN run --nv --cleanenv \\ $CONTAINER \\ mpirun \\ -H thetagpu01:1,thetagpu02:1 \\ -n 2 \\ --mca plm_rsh_args \"-p22\" \\ hostname","title":"Launching a Singularity container with MPI"},{"location":"Building_Compiling/","text":"Description These are the steps to build code that has Python/C++ code interoperability. 1. Login to a ThetaGPU head node ssh thetagpusn1 Request an interactive session on an A100 GPU qsub -n 1 -q default -A datascience -I -t 1:00:00 Following this, we need to execute a few commands to get setup with an appropriately optimized tensorflow. These are: 3. Activate the TensorFlow 2.2 singularity container: singularity exec -B /lus:/lus --nv /lus/theta-fs0/projects/datascience/thetaGPU/containers/tf2_20.08-py3.sif bash Setup access to the internet export HTTP_PROXY = http : // theta - proxy . tmi . alcf . anl . gov : 3128 export HTTPS_PROXY = https : // theta - proxy . tmi . alcf . anl . gov : 3128 Now that we can access the internet, we need to set up a virtual environment in Python (these commands should only be run the first time) python - m pip install -- user virtualenv export VENV_LOCATION =/ home / rmaulik / THETAGPU_TF_ENV # Add your path here python - m virtualenv -- system - site - packages $ VENV_LOCATION source $ VENV_LOCATION / bin / activate python - m pip install cmake python - m pip install matplotlib python - m pip install sklearn cmake is required to build our C++ app and link to Python, and other packages may be pip installed as needed in your Python code. An example CMakeLists.txt file for building with Python/C interoperability with examples can be found here .","title":"Description"},{"location":"Building_Compiling/#description","text":"These are the steps to build code that has Python/C++ code interoperability. 1. Login to a ThetaGPU head node ssh thetagpusn1 Request an interactive session on an A100 GPU qsub -n 1 -q default -A datascience -I -t 1:00:00 Following this, we need to execute a few commands to get setup with an appropriately optimized tensorflow. These are: 3. Activate the TensorFlow 2.2 singularity container: singularity exec -B /lus:/lus --nv /lus/theta-fs0/projects/datascience/thetaGPU/containers/tf2_20.08-py3.sif bash Setup access to the internet export HTTP_PROXY = http : // theta - proxy . tmi . alcf . anl . gov : 3128 export HTTPS_PROXY = https : // theta - proxy . tmi . alcf . anl . gov : 3128 Now that we can access the internet, we need to set up a virtual environment in Python (these commands should only be run the first time) python - m pip install -- user virtualenv export VENV_LOCATION =/ home / rmaulik / THETAGPU_TF_ENV # Add your path here python - m virtualenv -- system - site - packages $ VENV_LOCATION source $ VENV_LOCATION / bin / activate python - m pip install cmake python - m pip install matplotlib python - m pip install sklearn cmake is required to build our C++ app and link to Python, and other packages may be pip installed as needed in your Python code. An example CMakeLists.txt file for building with Python/C interoperability with examples can be found here .","title":"Description"},{"location":"ml_frameworks/tensorflow/nvidia_container_notes/","text":"NVidia Container Notes Getting the container To get NVidia docker containers which have the latest CUDA and Tensorflow installed, go to NVidia NGC , create an account, search for Tensorflow . Notice there are containers tagged with tf1 and tf2 . The page tells you how to select the right one. You can convert the command at the top, for instance: docker pull nvcr.io/nvidia/tensorflow:20.08-tf2-py3 to a singularity command by doing this: singularity build tensorflow-20.08-tf2-py3.simg docker://nvcr.io/nvidia/tensorflow:20.08-tf2-py3 You'll need to run this command on a Theta login node which has network access ( thetaloginX ). The containers from August, 2020, are also all available converted to singularity here: /lus/theta-fs0/projects/datascience/thetaGPU/containers/ Running on ThetaGPU After logging into ThetaGPU with ssh thetagpusn1 , one can submit job using the container one a single node by doing: qsub -n 1 -t 10 -A <project-name> submit.sh where submit.sh contians the following bash scripting: #!/bin/bash CONTAINER = $HOME /tensorflow-20.08-tf2-py3.simg singularity exec --nv $CONTAINER python /usr/local/lib/python3.6/dist-packages/tensorflow/python/debug/examples/debug_mnist.py make sure to make the script executable with chmod a+x submit.sh . The log file <cobalt-jobid>.output should contain some text like this: Accuracy at step 0 : 0 .2159 Accuracy at step 1 : 0 .098 Accuracy at step 2 : 0 .098 Accuracy at step 3 : 0 .098 Accuracy at step 4 : 0 .098 Accuracy at step 5 : 0 .098 Accuracy at step 6 : 0 .098 Accuracy at step 7 : 0 .098 Accuracy at step 8 : 0 .098 Accuracy at step 9 : 0 .098 The numbers may be different. Running Tensorflow-2 with Horovod on ThetaGPU To run on ThetaGPU with MPI you can do the follow test: git clone git@github.com:jtchilders/tensorflow_skeleton.git cd tensorflow_skeleton qsub -n 2 -t 20 -A <project-name> submit_scripts/thetagpu_mnist.sh You can inspect the submit script for details on how the job is constructed.","title":"Running with Singularity"},{"location":"ml_frameworks/tensorflow/nvidia_container_notes/#nvidia-container-notes","text":"","title":"NVidia Container Notes"},{"location":"ml_frameworks/tensorflow/nvidia_container_notes/#getting-the-container","text":"To get NVidia docker containers which have the latest CUDA and Tensorflow installed, go to NVidia NGC , create an account, search for Tensorflow . Notice there are containers tagged with tf1 and tf2 . The page tells you how to select the right one. You can convert the command at the top, for instance: docker pull nvcr.io/nvidia/tensorflow:20.08-tf2-py3 to a singularity command by doing this: singularity build tensorflow-20.08-tf2-py3.simg docker://nvcr.io/nvidia/tensorflow:20.08-tf2-py3 You'll need to run this command on a Theta login node which has network access ( thetaloginX ). The containers from August, 2020, are also all available converted to singularity here: /lus/theta-fs0/projects/datascience/thetaGPU/containers/","title":"Getting the container"},{"location":"ml_frameworks/tensorflow/nvidia_container_notes/#running-on-thetagpu","text":"After logging into ThetaGPU with ssh thetagpusn1 , one can submit job using the container one a single node by doing: qsub -n 1 -t 10 -A <project-name> submit.sh where submit.sh contians the following bash scripting: #!/bin/bash CONTAINER = $HOME /tensorflow-20.08-tf2-py3.simg singularity exec --nv $CONTAINER python /usr/local/lib/python3.6/dist-packages/tensorflow/python/debug/examples/debug_mnist.py make sure to make the script executable with chmod a+x submit.sh . The log file <cobalt-jobid>.output should contain some text like this: Accuracy at step 0 : 0 .2159 Accuracy at step 1 : 0 .098 Accuracy at step 2 : 0 .098 Accuracy at step 3 : 0 .098 Accuracy at step 4 : 0 .098 Accuracy at step 5 : 0 .098 Accuracy at step 6 : 0 .098 Accuracy at step 7 : 0 .098 Accuracy at step 8 : 0 .098 Accuracy at step 9 : 0 .098 The numbers may be different.","title":"Running on ThetaGPU"},{"location":"ml_frameworks/tensorflow/nvidia_container_notes/#running-tensorflow-2-with-horovod-on-thetagpu","text":"To run on ThetaGPU with MPI you can do the follow test: git clone git@github.com:jtchilders/tensorflow_skeleton.git cd tensorflow_skeleton qsub -n 2 -t 20 -A <project-name> submit_scripts/thetagpu_mnist.sh You can inspect the submit script for details on how the job is constructed.","title":"Running Tensorflow-2 with Horovod on ThetaGPU"},{"location":"ml_frameworks/tensorflow/running_with_conda/","text":"Running Tensorflow with Conda Beware that these builds use CUDA and will not work on login nodes, which does not have CUDA installed as there are no GPUs. Tensorflow (master build) Given A100 and CUDA 11 are very new, we have a build of the master branch of Tensorflow which includes better performance and support for these architectures. Users can utilize them by running this setup script: source /lus/theta-fs0/software/thetagpu/conda/tf_master/latest/mconda3/setup.sh This will setup a conda environment with a recent \"from scratch\" build of the Tensorflow repository on the master branch. The latest in the path is a symlink to a directory named by date that will be used to track our local builds. Per the writing of this documetation the only build uses latest points to 2020-11 . In the future, there will be newer builds available in that directory /lus/theta-fs0/software/thetagpu/conda/tf_master/ so check there for newer installs and run the respective mconda3/setup.sh script to use it. If you find things break since the last time you ran, it may be because latest is now pointing at a newer Tensorflow build. This package will also include the latest Horovod tagged release. Installing Packages Using pip install --user With the conda environment setup, one can install common Python modules using pip install --users <module-name> which will install packages in $HOME/.local/lib/pythonX.Y/site-packages . Using Conda Environments If you need more flexibility, you can clone the conda environment into a custom path, which would then allow for root-like installations via conda install <module> or pip install <module> . First, setup the conda environment you want to use as instructed above. Second, clone the environment into a local path to which you have write access conda create --clone $CONDA_PREFIX -p <path/to/env> Then activate that environment: conda activate <path/to/env> One should then be able to install modules freely.","title":"Running with Conda"},{"location":"ml_frameworks/tensorflow/running_with_conda/#running-tensorflow-with-conda","text":"Beware that these builds use CUDA and will not work on login nodes, which does not have CUDA installed as there are no GPUs.","title":"Running Tensorflow with Conda"},{"location":"ml_frameworks/tensorflow/running_with_conda/#tensorflow-master-build","text":"Given A100 and CUDA 11 are very new, we have a build of the master branch of Tensorflow which includes better performance and support for these architectures. Users can utilize them by running this setup script: source /lus/theta-fs0/software/thetagpu/conda/tf_master/latest/mconda3/setup.sh This will setup a conda environment with a recent \"from scratch\" build of the Tensorflow repository on the master branch. The latest in the path is a symlink to a directory named by date that will be used to track our local builds. Per the writing of this documetation the only build uses latest points to 2020-11 . In the future, there will be newer builds available in that directory /lus/theta-fs0/software/thetagpu/conda/tf_master/ so check there for newer installs and run the respective mconda3/setup.sh script to use it. If you find things break since the last time you ran, it may be because latest is now pointing at a newer Tensorflow build. This package will also include the latest Horovod tagged release.","title":"Tensorflow (master build)"},{"location":"ml_frameworks/tensorflow/running_with_conda/#installing-packages","text":"","title":"Installing Packages"},{"location":"ml_frameworks/tensorflow/running_with_conda/#using-pip-install-user","text":"With the conda environment setup, one can install common Python modules using pip install --users <module-name> which will install packages in $HOME/.local/lib/pythonX.Y/site-packages .","title":"Using pip install --user"},{"location":"ml_frameworks/tensorflow/running_with_conda/#using-conda-environments","text":"If you need more flexibility, you can clone the conda environment into a custom path, which would then allow for root-like installations via conda install <module> or pip install <module> . First, setup the conda environment you want to use as instructed above. Second, clone the environment into a local path to which you have write access conda create --clone $CONDA_PREFIX -p <path/to/env> Then activate that environment: conda activate <path/to/env> One should then be able to install modules freely.","title":"Using Conda Environments"},{"location":"ml_frameworks/tensorflow/tensorboard_instructions/","text":"Tensorboard Instructions After you have logged into ThetaGPU, and have a Tensorflow run going, you'll need to know one of your worker nodes so you can SSH to it. PORT0 = 9991 PORT1 = 9992 PORT3 = 9993 # Select a theta login node N where N=[1-6] ssh -L $PORT0 :localhost: $PORT1 $USER @thetaloginN.alcf.anl.gov # after reaching thetaloginN # Replace NN with your thetagpu worker node ssh -L $PORT1 :thetagpuNN: $PORT3 $USER @thetagpusn1 # after reaching thetagpusn1 # login to worker node ssh thetagpuNN # now setup your tensorflow environment # for instance run the conda setup.sh script created during the install_tensorflow.sh script # now run tensorboard tensorboard --logdir </path/to/logs> --port $PORT3 --bind_all","title":"Tensorboard"},{"location":"ml_frameworks/tensorflow/tensorboard_instructions/#tensorboard-instructions","text":"After you have logged into ThetaGPU, and have a Tensorflow run going, you'll need to know one of your worker nodes so you can SSH to it. PORT0 = 9991 PORT1 = 9992 PORT3 = 9993 # Select a theta login node N where N=[1-6] ssh -L $PORT0 :localhost: $PORT1 $USER @thetaloginN.alcf.anl.gov # after reaching thetaloginN # Replace NN with your thetagpu worker node ssh -L $PORT1 :thetagpuNN: $PORT3 $USER @thetagpusn1 # after reaching thetagpusn1 # login to worker node ssh thetagpuNN # now setup your tensorflow environment # for instance run the conda setup.sh script created during the install_tensorflow.sh script # now run tensorboard tensorboard --logdir </path/to/logs> --port $PORT3 --bind_all","title":"Tensorboard Instructions"}]}